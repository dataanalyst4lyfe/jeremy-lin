{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the following is to obtain the blog raw text & tags associated with the blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "\n",
    "# Constants\n",
    "RAW_SQL_DATA = \"RAW SQL data.txt\"\n",
    "BLOG_TAG_FILENAME = \"blog and tags.txt\"\n",
    "NON_ENGLISH_BLOG_TAG_FILENAME = \"non-English blog and tags.txt\"\n",
    "\n",
    "# https://stackoverflow.com/questions/11066400/remove-punctuation-from-unicode-formatted-strings/11066687#11066687\n",
    "PUNCTUATION_UNICODE_TABLE = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORD_LIST = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def removeHtmlCss(html):\n",
    "    \"\"\"\n",
    "    Description: This function cleans up an input string by removing the HTML tags from it.\n",
    "    \n",
    "    Source: https://stackoverflow.com/questions/22799990/beatifulsoup4-get-text-still-has-javascript\n",
    "    https://stackoverflow.com/questions/6467043/extracting-element-and-insert-a-space\n",
    "    ^ I had to figure that out because URLs would be meshed up with future lines.\n",
    "    e.g. \"found at http://www.tritonsvc.com.</p><p><strong><em>About\"\n",
    "    Would become \"found at http://www.tritonsvc.com.About\" without the (separator=u' ')\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html5lib\") # create a new bs4 object from the html data loaded\n",
    "\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "        # script.decompose()\n",
    "        \n",
    "    # Adds space after removing HTML/CSS tags\n",
    "    # Todo: Figure out way to put words back together\n",
    "    text = soup.get_text(separator=u'')\n",
    "\n",
    "    return text\n",
    "\n",
    "# To install langdetect: pip install langdetect\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import ErrorCode, LangDetectException\n",
    "\n",
    "# This function takes in an input string, and checks if most of that string is in English or not\n",
    "def detectLanguage(text):\n",
    "    \"\"\"\n",
    "    Description: This function returns a list of possible languages that an input string can be\n",
    "    \"\"\"\n",
    "    # Detect if string contains mostly non-Latin characters\n",
    "    chinese_character_count = 0\n",
    "    for ch in text:\n",
    "        if ord(ch) > 0x4e00 and ord(ch) < 0x9fff:\n",
    "            #print(\"Chinese: \" + str(ch))\n",
    "            chinese_character_count += 1\n",
    "\n",
    "    chinese_percentage = chinese_character_count / (len(text.split()) + chinese_character_count)\n",
    "    \n",
    "    if chinese_percentage > 0:\n",
    "        print(\"Chinese Percentage: \" + str(chinese_percentage) + \", \" + str(title))\n",
    "    \n",
    "    if chinese_percentage > 0 and chinese_percentage < 0.50:\n",
    "        print(\"Warning - Unsure if Chinese! Assumming it is not Chinese!\")\n",
    "        #print(\"Chinese Percentage: \" + str(chinese_percentage) + \", \" + str(title))\n",
    "        print(str(text))\n",
    "        \n",
    "    elif chinese_percentage > 0.50:\n",
    "        print(\"Probably Chinese\")\n",
    "        #print(\"Chinese Percentage: \" + str(chinese_percentage) + \", \" + str(title))\n",
    "        return ['zh:{}'.format(chinese_percentage)]\n",
    "    \n",
    "    try:\n",
    "        # Check for other foreign languages\n",
    "        language_dector_result = detect(text)\n",
    "\n",
    "        possible_language_list = langdetect.detect_langs(text)\n",
    "        if len(possible_language_list) >= 2:\n",
    "            print(text)\n",
    "            print(possible_language_list)\n",
    "\n",
    "        # print(language_dector_results)\n",
    "        # if language_dector_results != 'en':\n",
    "        #    print(language_dector_results)\n",
    "\n",
    "        return possible_language_list\n",
    "\n",
    "    except LangDetectException:\n",
    "        print(\"Empty Blog Entry?\")\n",
    "        #print(\"Title: {}\\nBlog: {}\\nTags: {}\\n\".format(title_line, blog_line[:100], tag_line))\n",
    "        print(\"Text: {}\".format(text))\n",
    "        print(\"\\n\\n\")\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def _remove_all_attrs(soup):\n",
    "    for tag in soup.find_all(True): \n",
    "        tag.attrs = {}\n",
    "    return soup \n",
    "    \n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "    \n",
    "def cleanUpText(original_text):\n",
    "    \"\"\"\n",
    "    Description: This function removes puncutation\n",
    "        \n",
    "    XXX: Add flags for \n",
    "        - removing numbers\n",
    "        - certain types of punctuation\n",
    "        - retaining lower/uper-case\n",
    "    \"\"\"\n",
    "    \n",
    "    modified_text = original_text\n",
    "\n",
    "    # Convert text to all lower-case\n",
    "    modified_text = modified_text.lower()\n",
    "    \n",
    "    # Remove URLs/File directories\n",
    "    modified_text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', modified_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove punctuation\n",
    "    modified_text = modified_text.translate(PUNCTUATION_UNICODE_TABLE)\n",
    "\n",
    "    # Remove numbers\n",
    "    modified_text = re.sub(r'\\d+', '', str(modified_text))\n",
    "\n",
    "    # Tokenize\n",
    "    # tokens = no_numbers.split()\n",
    "    \n",
    "    # Remove code\n",
    "    \n",
    "    \n",
    "    \n",
    "    return modified_text\n",
    "    \n",
    "def printMostCommonWords(tokens):\n",
    "    \"\"\"\n",
    "    Graphs the 50 most common tokens given\n",
    "    \"\"\"\n",
    "    \n",
    "    ##############\n",
    "    # Graph Data #\n",
    "    ##############\n",
    "    import nltk\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # This is to make the graph bigger\n",
    "    plt.figure(figsize=(15, 5))  # the size you want\n",
    "\n",
    "    # Remove stopwords according to NLTK\n",
    "    filtered_words = [w for w in tokens if not w in STOPWORD_LIST]\n",
    "\n",
    "    fdist2 = nltk.FreqDist(filtered_words)\n",
    "    fdist2.plot(50, cumulative=False)\n",
    "    \n",
    "    print(fdist2.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "cnx = mysql.connector.connect(user='root', password='liferay',\n",
    "                              host='127.0.0.1',\n",
    "                              database='sample_customer_data')\n",
    "cnx.close()\n",
    "\n",
    "try:\n",
    "    cnx = mysql.connector.connect(user='root', password='liferay',\n",
    "                              host='127.0.0.1',\n",
    "                              database='sample_developer_data',\n",
    "                              charset='utf8')\n",
    "\n",
    "    cursor = cnx.cursor(buffered=True)\n",
    "    cursor2 = cnx.cursor()\n",
    "\n",
    "    query = (\"SELECT blogsentry.title, blogsentry.content, assetentry.entryId FROM sample_developer_data.assetentry \\\n",
    "                INNER JOIN sample_developer_data.blogsentry \\\n",
    "                ON sample_developer_data.assetentry.classPK = sample_developer_data.blogsentry.entryId;\")\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    #print(cursor)\n",
    "    \n",
    "    counter_with_content = 0\n",
    "    \n",
    "    file = open(BLOG_TAG_FILENAME, \"w\", encoding='utf8')\n",
    "    non_english_file = open(NON_ENGLISH_BLOG_TAG_FILENAME, \"w\", encoding='utf8')\n",
    "\n",
    "    for (title, content, entryId) in cursor:\n",
    "        \n",
    "        # Cleanup Text\n",
    "        clean_title = removeHtmlCss(title)\n",
    "        clean_title = clean_title.replace(\"]]>\", \"\")\n",
    "        clean_content = removeHtmlCss(content)\n",
    "        clean_content = clean_content.replace(\"]]>\", \"\")\n",
    "        #clean_content = content\n",
    "        \n",
    "        # Run the HTML CSS cleanup again, because of nested HTML/CSS/JS code\n",
    "        #clean_content = removeHtmlCss(clean_content)\n",
    "\n",
    "        #print(notags)\n",
    "        \n",
    "        \n",
    "        # Reduce all whitespace to a single space\n",
    "        clean_title = \" \".join(clean_title.split()).strip()\n",
    "        clean_content = \" \".join(clean_content.split()).strip()\n",
    "                \n",
    "        query2 = (\"SELECT assettag.name FROM sample_developer_data.assettag \\\n",
    "                      WHERE sample_developer_data.assettag.tagId \\\n",
    "                      IN (SELECT assetentries_assettags.tagId \\\n",
    "                        FROM sample_developer_data.assetentries_assettags \\\n",
    "                        WHERE sample_developer_data.assetentries_assettags.entryId = \" + str(entryId) + \")\")\n",
    "\n",
    "        cursor2.execute(query2)\n",
    "        tag_list = list()\n",
    "        \n",
    "        for (name) in cursor2:\n",
    "            tag_list.append(name[0])\n",
    "        \n",
    "        # Verify that tags exist & there's content\n",
    "        if tag_list and clean_content:\n",
    "            \n",
    "            # Verify content is in English\n",
    "            content_language = detectLanguage(clean_content)\n",
    "            \n",
    "            print(content_language)\n",
    "            \n",
    "            lang = str(content_language[0]).split(':')\n",
    "            \n",
    "            # if ('en' in content_language) and (content_language.count(':') <= 1):\n",
    "            if ((lang[0] == 'en') and (float(lang[1]) > .80)):\n",
    "                counter_with_content += 1\n",
    "\n",
    "                print(\"\\n{}) {}\\n{}\\n{}\\n\\n\".format(counter, clean_title, clean_content[:50], tag_list))\n",
    "\n",
    "                output = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_content, \",\".join(tag_list))\n",
    "                output.encode('utf8')\n",
    "                # print(output)\n",
    "                file.write(output)\n",
    "            else:\n",
    "                output = \"{}\\n{}\\n{}\\n{}\\n\\n\".format(title, clean_content, \",\".join(tag_list), content_language)\n",
    "                output.encode('utf8')\n",
    "                non_english_file.write(output)\n",
    "                continue\n",
    "                \n",
    "        else:\n",
    "            #print(\"{}) Empty String\".format(counter))\n",
    "            #output = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_content, \",\".join(tag_list))\n",
    "            pass\n",
    "            \n",
    "        \n",
    "        #print(content[0].strip())\n",
    "        counter += 1\n",
    "        \n",
    "        if counter % 500 == 0:\n",
    "            print(\"Current Count: \" + str(counter))\n",
    "            pass\n",
    "\n",
    "    file.close()\n",
    "    non_english_file.close()\n",
    "    print(\"Total Entries: {}\\nContent Entries: {}\".format(counter, counter_with_content))\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "    print(\"Something is wrong with your user name or password\")\n",
    "  elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "    print(\"Database does not exist\")\n",
    "  else:\n",
    "    print(err)\n",
    "else:\n",
    "  cnx.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will open the blog & tags file, and cleanup whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langdetect import detect_langs\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import ErrorCode, LangDetectException\n",
    "\n",
    "title_list = list()\n",
    "blog_list = list()\n",
    "tag_list = list()\n",
    "\n",
    "blog_tag_file = open(BLOG_TAG_FILENAME, 'r', encoding='utf-8')\n",
    "    \n",
    "while True:\n",
    "    title_line = blog_tag_file.readline().strip()\n",
    "    blog_line = blog_tag_file.readline().strip()\n",
    "    tag_line = blog_tag_file.readline().strip()\n",
    "    \n",
    "    try:\n",
    "        #langauge = langdetect.detect_langs(blog_line)\n",
    "        language = detect(blog_line)\n",
    "    except LangDetectException:\n",
    "        print(\"Empty Blog Entry?\")\n",
    "        print(\"Title: {}\\nBlog: {}\\nTags: {}\\n\".format(title_line, blog_line[:100], tag_line))\n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    blog_tag_file.readline() # This is just to burn the extra new line\n",
    "    #print(\"Title: {}\\nBlog: {}\\nTags: {}\\n\".format(title_line, blog_line[:100], tag_line))\n",
    "    #print(\"Title: {} - {} {}\\n\".format(title_line, blog_line[100:], langauge))\n",
    "    \n",
    "    # We only care about entries that have blog entries\n",
    "    if tag_line:\n",
    "        title_list.append(title_line)\n",
    "        blog_list.append(blog_line)\n",
    "        tag_list.append(tag_line)\n",
    "    else:\n",
    "        print(\"End of File\")\n",
    "        print(\"Title: {}\\nBlog: {}\\nTags: {}\\n\".format(title_line, blog_line[:100], tag_line))\n",
    "        break # EOF\n",
    "\n",
    "print(blog_list[0][:100])\n",
    "print(blog_list[len(title_list) - 1][:100])\n",
    "print(len(title_list) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write back to file with:\n",
    "# - HTML removed\n",
    "# - Remove URLs (http, https, ftp, svn)\n",
    "\n",
    "import re\n",
    "\n",
    "# Note: This is a very naive implementation, there is probably a better way to do this.\n",
    "# This regex is to remove any URLs\n",
    "regex = re.compile(r\"(http|ftp|https|svn):\\/\\/[\\S]+\", re.IGNORECASE)\n",
    "\n",
    "file1 = open('CLEANUP 1 - Remove HTML and CSS.txt', \"w\", encoding='utf8')\n",
    "file2 = open('CLEANUP 2 - Remove URLs.txt', \"w\", encoding='utf8')\n",
    "file3 = open('CLEANUP 3 - Remove Puncuation.txt', \"w\", encoding='utf8')\n",
    "file4 = open('CLEANUP 4 - Remove Numbers.txt', \"w\", encoding='utf8')\n",
    "file5 = open('CLEANUP 5 - Convert to Lower Case.txt', \"w\", encoding='utf8')\n",
    "file6 = open('CLEANUP 6 - Adjust Whitespace.txt', \"w\", encoding='utf8')\n",
    "\n",
    "for index in range(len(title_list)):\n",
    "    title = title_list[index]\n",
    "    blog = blog_list[index]\n",
    "    tags = tag_list[index]\n",
    "    \n",
    "    # Cleanup blog contents\n",
    "\n",
    "    # Remove HTML, CSS, & JS\n",
    "    clean_blog = removeHtmlCss(blog)\n",
    "    output_string = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_blog, tags)\n",
    "    file1.write(output_string)\n",
    "    \n",
    "    # Remove URLs\n",
    "    clean_blog = regex.sub(\"\", clean_blog)\n",
    "    output_string = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_blog, tags)\n",
    "    file2.write(output_string)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    clean_blog = clean_blog.translate(PUNCTUATION_UNICODE_TABLE)\n",
    "    output_string = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_blog, tags)\n",
    "    file3.write(output_string)\n",
    "    \n",
    "    # Remove numbers\n",
    "    #clean_blog = re.sub(r'\\d+', '', str(clean_blog))\n",
    "    #output_string = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_blog, tags)\n",
    "    #file4.write(output_string)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    clean_blog = clean_blog.lower()\n",
    "    output_string = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_blog, tags)\n",
    "    file5.write(output_string)\n",
    "    \n",
    "    # Adjust whitespace to only single spaces\n",
    "    clean_blog = \" \".join(clean_blog.split()).strip()\n",
    "    output_string = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_blog, tags)\n",
    "    file6.write(output_string)\n",
    "    \n",
    "    # output_string = \"{}\\n{}\\n{}\\n\\n\".format(title, clean_blog, tags)\n",
    "    \n",
    "    #file.write(output_string)\n",
    "    \n",
    "    # Update array contents\n",
    "    #title_list[index] = \n",
    "    blog_list[index] = clean_blog\n",
    "    #tag_list[index]\n",
    "\n",
    "#file.close()\n",
    "\n",
    "print(blog_list[0][:100])\n",
    "print(blog_list[len(title_list) - 1][:100])\n",
    "print(len(title_list) - 1)\n",
    "\n",
    "file1.close()\n",
    "file2.close()\n",
    "file3.close()\n",
    "file4.close()\n",
    "file5.close()\n",
    "file6.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Print most common words in all the blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in cleaned words from last file\n",
    "\n",
    "cleaned_blog_list = list()\n",
    "\n",
    "file6 = open('CLEANUP 6 - Adjust Whitespace.txt', \"r\", encoding='utf8')\n",
    "    \n",
    "while True:\n",
    "    title_line = file6.readline().strip()\n",
    "    blog_line = file6.readline().strip()\n",
    "    tag_line = file6.readline().strip()\n",
    "        \n",
    "    file6.readline() # This is just to burn the extra new line\n",
    "    #print(\"Title: {}\\nBlog: {}\\nTags: {}\\n\".format(title_line, blog_line[:100], tag_line))\n",
    "    \n",
    "    # We only care about entries that have blog entries\n",
    "    if title_line:\n",
    "        #title_list.append(title_line)\n",
    "        cleaned_blog_list.append(blog_line)\n",
    "        #tag_list.append(tag_line)\n",
    "    else:\n",
    "        print(\"End of File\")\n",
    "        print(\"Title: {}\\nBlog: {}\\nTags: {}\\n\".format(title_line, blog_line[:100], tag_line))\n",
    "        break # EOF\n",
    "\n",
    "# Tokenize\n",
    "tokens = ' '.join(cleaned_blog_list).split()\n",
    "\n",
    "# Graph words\n",
    "printMostCommonWords(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating frequency of words on a per document basis. (i.e. if a document is in every document, then it has the highest frequency possible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Build vocabulary set\n",
    "\n",
    "vocab_document_count = defaultdict(lambda: 0)\n",
    "\n",
    "for blog_words in blog_list:\n",
    "    tokens = blog_words.split()\n",
    "    vocab_set = set(tokens)\n",
    "    \n",
    "    # print('\\n' + blog_words[:50])\n",
    "    \n",
    "    # Iterate through set, and update the vocab_document_count\n",
    "    for word in vocab_set:\n",
    "        vocab_document_count[word] += int(1)\n",
    "        \n",
    "    # Debugging: Printing all the values in the dictionary\n",
    "    #for word, count in vocab_document_count.items():\n",
    "    #    print(\"{} : {}\".format(word, count))\n",
    "    \n",
    "    #break\n",
    "\n",
    "# Debugging: Printing all the values in the dictionary\n",
    "#for word, count in vocab_document_count.items():\n",
    "#        print(\"{} : {}\".format(word, count))\n",
    "        \n",
    "vocab_list = list(vocab_document_count)\n",
    "word_index = {w: idx for idx, w in enumerate(vocab_document_count)}\n",
    "\n",
    "VOCABULARY_SIZE = len(vocab_list)\n",
    "DOCUMENTS_COUNT = len(blog_list)\n",
    " \n",
    "print(\"Vocab Size: \" + str(VOCABULARY_SIZE) + \" Document Count: \" + str(DOCUMENTS_COUNT))      # 10788, 51581\n",
    "\n",
    "for w in sorted(vocab_document_count, key=vocab_document_count.get, reverse=True):\n",
    "    print(\"{} : {}\".format(w, vocab_document_count[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup the data by removing stopwords, or haplax legomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Low Frequency & High Frequency Words\n",
    "\n",
    "low_frequency_word_list = list()\n",
    "high_frequency_word_list = list()\n",
    "\n",
    "LOW_FREQUENCY_LIMIT_INCLUSIVE = 4\n",
    "HIGH_FREQUENCY_LIMIT_INCLUSIVE = 500\n",
    "\n",
    "unique_words_before = len(vocab_document_count)\n",
    "\n",
    "# Iterate through document frequency dictionary, and remove all low frequency instances\n",
    "for word in list(vocab_document_count.keys()):\n",
    "    \n",
    "    if vocab_document_count[word] <= LOW_FREQUENCY_LIMIT_INCLUSIVE:\n",
    "        del vocab_document_count[word]\n",
    "        low_frequency_word_list.append(word)\n",
    "        \n",
    "unique_words_after = len(vocab_document_count)\n",
    "\n",
    "print(\"Length (before): {}, Length (after): {}\".format(unique_words_before, unique_words_after))\n",
    "    \n",
    "# vocab_document_count = vocab_document_count_removed_low_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining Q1, Q2, Q3, Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_entries = len(vocab_document_count)\n",
    "max_index = number_of_entries - 1\n",
    "min_index = 0\n",
    "q2_index = int(number_of_entries / 2)\n",
    "q1_index = int(q2_index / 2)\n",
    "q3_index = q2_index + q1_index\n",
    "\n",
    "sorted_word_list = sorted(vocab_document_count, key=vocab_document_count.get, reverse=False)\n",
    "\n",
    "print(\"Min: {} ({})\".format(sorted_word_list[min_index], vocab_document_count[sorted_word_list[min_index]]))\n",
    "print(\"Q1:  {} ({})\".format(sorted_word_list[q1_index], vocab_document_count[sorted_word_list[q1_index]]))\n",
    "print(\"Q2:  {} ({})\".format(sorted_word_list[q2_index], vocab_document_count[sorted_word_list[q2_index]]))\n",
    "print(\"Q3:  {} ({})\".format(sorted_word_list[q3_index], vocab_document_count[sorted_word_list[q3_index]]))\n",
    "print(\"Q4:  {} ({})\".format(sorted_word_list[max_index], vocab_document_count[sorted_word_list[max_index]]))\n",
    "      \n",
    "      #Max: {} ({})\".format(vocab_document_count[min_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Data\n",
    "\n",
    "#plt.bar(range(len(vocab_document_count)), vocab_document_count.values(), align='center')\n",
    "#plt.xticks(range(len(vocab_document_count)), vocab_document_count.keys())\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "# Calculate Stats on the distro\n",
    "###############################\n",
    "document_frequency_list = list(vocab_document_count.values())\n",
    "\n",
    "# Calcuate Mean, Median, Stdev\n",
    "import numpy as np\n",
    "\n",
    "mean = np.mean(document_frequency_list)\n",
    "median = np.median(document_frequency_list)\n",
    "std_dev = np.std(document_frequency_list)\n",
    "\n",
    "print(\"Mean: {}\\nMedian: {}\\nStandard Deviation: {}\".format(mean, median, std_dev))\n",
    "\n",
    "# Graph data\n",
    "############\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Clear previous instances\n",
    "plt.clf()\n",
    "\n",
    "# Create a figure instance\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Create the boxplot\n",
    "bp = ax.boxplot(document_frequency_list)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('Vocab Frequency by Documents.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate (Inverse) Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_df = defaultdict(lambda: 0)\n",
    "\n",
    "for blog_words in blog_list:\n",
    "    tokens = blog_words.split()\n",
    "    word_set = set(tokens)\n",
    "    \n",
    "    for word in word_set:\n",
    "        word_df[word] += 1\n",
    "\n",
    "import math\n",
    "for word in vocab_list:\n",
    "    if word_df[word] >= 2:\n",
    "        #word_df[word] = math.log((word_df[word] + 1) / len(blog_list))\n",
    "        word_df[word] = word_df[word] / len(blog_list)\n",
    "    \n",
    "#counter = 0\n",
    "#for word in word_df:\n",
    "#    print(\"Word: {} - {}\".format(word, word_df[word]))\n",
    "    \n",
    "#    if counter > 10:\n",
    "#        break\n",
    "#    else:\n",
    "#        counter += 1\n",
    "        \n",
    "sorted_list = sorted(word_df.items(), key=lambda x:x[1])\n",
    "\n",
    "word_list = []\n",
    "log_score_list = []\n",
    "\n",
    "raw_count_list = []\n",
    "\n",
    "for entry in sorted_list:\n",
    "    print(\"{} - {}\".format(entry[0], entry[1]))\n",
    "    word_list.append(entry[0])\n",
    "    log_score_list.append(entry[1])\n",
    "    \n",
    "    if entry[1] >= 2:\n",
    "        raw_count_list.append(entry[1])\n",
    "    \n",
    "#word_list = [x[0] for x in sorted_list]\n",
    "#log_score_list = [x[1] for x in sorted_list]\n",
    "\n",
    "#print(word_list[2])\n",
    "#print(word_list[3])\n",
    "#print(log_score_list[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate key statistical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate Mean, Median, Stdev\n",
    "import numpy as np\n",
    "\n",
    "mean = np.mean(log_score_list)\n",
    "median = np.median(log_score_list)\n",
    "std_dev = np.std(log_score_list)\n",
    "\n",
    "print(\"Mean: {}\\nMedian: {}\\nStandard Deviation: {}\".format(mean, median, std_dev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to graph the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Create a figure instance\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Create the boxplot\n",
    "bp = ax.boxplot(log_score_list)\n",
    "#bp = ax.boxplot(raw_count_list[:500])\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('fig1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_content = \"tu madre su se your momma is so fat\"            \n",
    "content_language = detectLanguage(clean_content)\n",
    "\n",
    "type(content_language)\n",
    "\n",
    "print(content_language[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = '作者 Bruno Farache英文原文 Web Content（以前叫Journal）最近发布一个新功能。'\n",
    "\n",
    "testing.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanUpText('hello. asdfljklajksdf;asfdljjlafsjioaewjr Timmy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"(http|ftp|https):\\/\\/[\\S]+\", re.IGNORECASE)\n",
    "test_string = \"http://www.edward.com\"\n",
    "\n",
    "\n",
    "regex.sub(\"\", test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeHtmlCss('<strong>1. Load Balance</strong>r - it can be software (i.e. - Apache), or hardware (F5), or whatever you wish, really. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_results = detectLanguage('hello my name is huan, es muy bieno')\n",
    "print(language_results)\n",
    "\n",
    "lang = str(language_results[0]).split(':')\n",
    "print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
